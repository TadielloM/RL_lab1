{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to lab 1  \n",
    "In this notebook, we use the following modules `numpy` and `maze`. The latter is a home made module, where all the solutions to the questions are implemented. We will refer to it at each answer, and we encourage you to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import maze1 as mz \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Shortest path in the maze\n",
    "\n",
    "The objective of problem 1 is to solve the shortest path problem in a maze. We start first by describing the maze as a numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "# with the convention \n",
    "# 0 = empty cell\n",
    "# 1 = obstacle\n",
    "# 2 = exit of the Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `maze.draw_maze()` helps us draw the maze given its numpy array discription.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"413.538125pt\" version=\"1.1\" viewBox=\"0 0 464.3 413.538125\" width=\"464.3pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 413.538125 \nL 464.3 413.538125 \nL 464.3 0 \nL 0 0 \nz\n\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 402.838125 \nL 457.1 402.838125 \nL 457.1 22.318125 \nL 10.7 22.318125 \nz\n\"/>\n   </g>\n   <g id=\"table_1\">\n    <g id=\"patch_3\">\n     <path d=\"M 10.7 76.678125 \nL 66.5 76.678125 \nL 66.5 22.318125 \nL 10.7 22.318125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_4\">\n     <path d=\"M 66.5 76.678125 \nL 122.3 76.678125 \nL 122.3 22.318125 \nL 66.5 22.318125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_5\">\n     <path d=\"M 122.3 76.678125 \nL 178.1 76.678125 \nL 178.1 22.318125 \nL 122.3 22.318125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_6\">\n     <path d=\"M 178.1 76.678125 \nL 233.9 76.678125 \nL 233.9 22.318125 \nL 178.1 22.318125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_7\">\n     <path d=\"M 233.9 76.678125 \nL 289.7 76.678125 \nL 289.7 22.318125 \nL 233.9 22.318125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_8\">\n     <path d=\"M 289.7 76.678125 \nL 345.5 76.678125 \nL 345.5 22.318125 \nL 289.7 22.318125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_9\">\n     <path d=\"M 345.5 76.678125 \nL 401.3 76.678125 \nL 401.3 22.318125 \nL 345.5 22.318125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_10\">\n     <path d=\"M 401.3 76.678125 \nL 457.1 76.678125 \nL 457.1 22.318125 \nL 401.3 22.318125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_11\">\n     <path d=\"M 10.7 131.038125 \nL 66.5 131.038125 \nL 66.5 76.678125 \nL 10.7 76.678125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_12\">\n     <path d=\"M 66.5 131.038125 \nL 122.3 131.038125 \nL 122.3 76.678125 \nL 66.5 76.678125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_13\">\n     <path d=\"M 122.3 131.038125 \nL 178.1 131.038125 \nL 178.1 76.678125 \nL 122.3 76.678125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_14\">\n     <path d=\"M 178.1 131.038125 \nL 233.9 131.038125 \nL 233.9 76.678125 \nL 178.1 76.678125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_15\">\n     <path d=\"M 233.9 131.038125 \nL 289.7 131.038125 \nL 289.7 76.678125 \nL 233.9 76.678125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_16\">\n     <path d=\"M 289.7 131.038125 \nL 345.5 131.038125 \nL 345.5 76.678125 \nL 289.7 76.678125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_17\">\n     <path d=\"M 345.5 131.038125 \nL 401.3 131.038125 \nL 401.3 76.678125 \nL 345.5 76.678125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_18\">\n     <path d=\"M 401.3 131.038125 \nL 457.1 131.038125 \nL 457.1 76.678125 \nL 401.3 76.678125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_19\">\n     <path d=\"M 10.7 185.398125 \nL 66.5 185.398125 \nL 66.5 131.038125 \nL 10.7 131.038125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_20\">\n     <path d=\"M 66.5 185.398125 \nL 122.3 185.398125 \nL 122.3 131.038125 \nL 66.5 131.038125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_21\">\n     <path d=\"M 122.3 185.398125 \nL 178.1 185.398125 \nL 178.1 131.038125 \nL 122.3 131.038125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_22\">\n     <path d=\"M 178.1 185.398125 \nL 233.9 185.398125 \nL 233.9 131.038125 \nL 178.1 131.038125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_23\">\n     <path d=\"M 233.9 185.398125 \nL 289.7 185.398125 \nL 289.7 131.038125 \nL 233.9 131.038125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_24\">\n     <path d=\"M 289.7 185.398125 \nL 345.5 185.398125 \nL 345.5 131.038125 \nL 289.7 131.038125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_25\">\n     <path d=\"M 345.5 185.398125 \nL 401.3 185.398125 \nL 401.3 131.038125 \nL 345.5 131.038125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_26\">\n     <path d=\"M 401.3 185.398125 \nL 457.1 185.398125 \nL 457.1 131.038125 \nL 401.3 131.038125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_27\">\n     <path d=\"M 10.7 239.758125 \nL 66.5 239.758125 \nL 66.5 185.398125 \nL 10.7 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_28\">\n     <path d=\"M 66.5 239.758125 \nL 122.3 239.758125 \nL 122.3 185.398125 \nL 66.5 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_29\">\n     <path d=\"M 122.3 239.758125 \nL 178.1 239.758125 \nL 178.1 185.398125 \nL 122.3 185.398125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_30\">\n     <path d=\"M 178.1 239.758125 \nL 233.9 239.758125 \nL 233.9 185.398125 \nL 178.1 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_31\">\n     <path d=\"M 233.9 239.758125 \nL 289.7 239.758125 \nL 289.7 185.398125 \nL 233.9 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_32\">\n     <path d=\"M 289.7 239.758125 \nL 345.5 239.758125 \nL 345.5 185.398125 \nL 289.7 185.398125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_33\">\n     <path d=\"M 345.5 239.758125 \nL 401.3 239.758125 \nL 401.3 185.398125 \nL 345.5 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_34\">\n     <path d=\"M 401.3 239.758125 \nL 457.1 239.758125 \nL 457.1 185.398125 \nL 401.3 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_35\">\n     <path d=\"M 10.7 294.118125 \nL 66.5 294.118125 \nL 66.5 239.758125 \nL 10.7 239.758125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_36\">\n     <path d=\"M 66.5 294.118125 \nL 122.3 294.118125 \nL 122.3 239.758125 \nL 66.5 239.758125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_37\">\n     <path d=\"M 122.3 294.118125 \nL 178.1 294.118125 \nL 178.1 239.758125 \nL 122.3 239.758125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_38\">\n     <path d=\"M 178.1 294.118125 \nL 233.9 294.118125 \nL 233.9 239.758125 \nL 178.1 239.758125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_39\">\n     <path d=\"M 233.9 294.118125 \nL 289.7 294.118125 \nL 289.7 239.758125 \nL 233.9 239.758125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_40\">\n     <path d=\"M 289.7 294.118125 \nL 345.5 294.118125 \nL 345.5 239.758125 \nL 289.7 239.758125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_41\">\n     <path d=\"M 345.5 294.118125 \nL 401.3 294.118125 \nL 401.3 239.758125 \nL 345.5 239.758125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_42\">\n     <path d=\"M 401.3 294.118125 \nL 457.1 294.118125 \nL 457.1 239.758125 \nL 401.3 239.758125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_43\">\n     <path d=\"M 10.7 348.478125 \nL 66.5 348.478125 \nL 66.5 294.118125 \nL 10.7 294.118125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_44\">\n     <path d=\"M 66.5 348.478125 \nL 122.3 348.478125 \nL 122.3 294.118125 \nL 66.5 294.118125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_45\">\n     <path d=\"M 122.3 348.478125 \nL 178.1 348.478125 \nL 178.1 294.118125 \nL 122.3 294.118125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_46\">\n     <path d=\"M 178.1 348.478125 \nL 233.9 348.478125 \nL 233.9 294.118125 \nL 178.1 294.118125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_47\">\n     <path d=\"M 233.9 348.478125 \nL 289.7 348.478125 \nL 289.7 294.118125 \nL 233.9 294.118125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_48\">\n     <path d=\"M 289.7 348.478125 \nL 345.5 348.478125 \nL 345.5 294.118125 \nL 289.7 294.118125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_49\">\n     <path d=\"M 345.5 348.478125 \nL 401.3 348.478125 \nL 401.3 294.118125 \nL 345.5 294.118125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_50\">\n     <path d=\"M 401.3 348.478125 \nL 457.1 348.478125 \nL 457.1 294.118125 \nL 401.3 294.118125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_51\">\n     <path d=\"M 10.7 402.838125 \nL 66.5 402.838125 \nL 66.5 348.478125 \nL 10.7 348.478125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_52\">\n     <path d=\"M 66.5 402.838125 \nL 122.3 402.838125 \nL 122.3 348.478125 \nL 66.5 348.478125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_53\">\n     <path d=\"M 122.3 402.838125 \nL 178.1 402.838125 \nL 178.1 348.478125 \nL 122.3 348.478125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_54\">\n     <path d=\"M 178.1 402.838125 \nL 233.9 402.838125 \nL 233.9 348.478125 \nL 178.1 348.478125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_55\">\n     <path d=\"M 233.9 402.838125 \nL 289.7 402.838125 \nL 289.7 348.478125 \nL 233.9 348.478125 \nz\n\" style=\"stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_56\">\n     <path d=\"M 289.7 402.838125 \nL 345.5 402.838125 \nL 345.5 348.478125 \nL 289.7 348.478125 \nz\n\" style=\"fill:#95fd99;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_57\">\n     <path d=\"M 345.5 402.838125 \nL 401.3 402.838125 \nL 401.3 348.478125 \nL 345.5 348.478125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"patch_58\">\n     <path d=\"M 401.3 402.838125 \nL 457.1 402.838125 \nL 457.1 348.478125 \nL 401.3 348.478125 \nz\n\" style=\"fill:#ffffff;stroke:#000000;stroke-linejoin:miter;\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\"/>\n   <g id=\"matplotlib.axis_2\"/>\n   <g id=\"patch_59\">\n    <path d=\"M 10.7 402.838125 \nL 10.7 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_60\">\n    <path d=\"M 457.1 402.838125 \nL 457.1 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_61\">\n    <path d=\"M 10.7 402.838125 \nL 457.1 402.838125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_62\">\n    <path d=\"M 10.7 22.318125 \nL 457.1 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- The Maze -->\n    <defs>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 5.515625 54.6875 \nL 48.1875 54.6875 \nL 48.1875 46.484375 \nL 14.40625 7.171875 \nL 48.1875 7.171875 \nL 48.1875 0 \nL 4.296875 0 \nL 4.296875 8.203125 \nL 38.09375 47.515625 \nL 5.515625 47.515625 \nz\n\" id=\"DejaVuSans-122\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(205.139375 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"61.083984\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"124.462891\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"185.986328\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"217.773438\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"304.052734\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"365.332031\" xlink:href=\"#DejaVuSans-122\"/>\n     <use x=\"417.822266\" xlink:href=\"#DejaVuSans-101\"/>\n    </g>\n   </g>\n  </g>\n </g>\n</svg>\n",
      "text/plain": "<Figure size 576x504 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mz.draw_maze(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP formulation\n",
    "\n",
    "We propose the following MDP formulation: \n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "We model the state space as the set of all possible positions of the player and minotaur in the maze. Note that we exclude the obstacles' position for the player since these are impossible states to be in for the player. Formally, the state space is\n",
    "\n",
    "$$\\mathcal{S} = \\big\\lbrace (i,j):\\textrm{such that the cell\n",
    "} (i,j) \\textrm{ is not an obstacle}\\big\\rbrace.$$\n",
    "> **Note:** The choice of state space is not unique. For instance one could consider $\\mathcal{S}$ to be the set of all positions in the  maze regardless of whether they correspond to an obstacle or not. But note that, this will increase the size $\\vert \\mathcal{S} \\vert $. This is fine for small mazes, but it leads to many redundant states as the maze dimension increases.\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "We allow the player to chose to either move `left`, `right`, `down`, `up` or not move at all (`stay`). Note that sometimes the player cannot move in a certain direction because of an obstacle or a wall, yet we permit this to be action. We will see that this is not an issue as long as we define our transition probabilities and rewards appropriately.\n",
    "Formally, the action space is\n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace.$$\n",
    "> **Note:** Once again, the choice of the action space is not unique. For instance one could remove the action `stay` from $\\mathcal{A}$, but then one should modify the transition probabilities accordingly as well as the rewards.  \n",
    "\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "Note that there is no randomness involved upon taking an action by the player. As a consequence, the transition probabilities are deterministic. More precisely,   \n",
    "- If at state (or position) $s$ taking action (or move) $a$ does not lead to a wall or an obstacle but to another state (or position) $s'$, then $\\mathbb{P}(s' \\vert s, a) = 1$. \n",
    "- If at state (or position)  $s$ taking action (or move) $a$ leads to a wall or an obstacle, the player remains in his state (or position) $s$, then $\\mathbb{P}(s \\vert s, a) = 1$.\n",
    "\n",
    "> **Note**: Recall that for a fixed $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$ we have $\\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s' \\vert s, a) = 1$, thus if for some $s' \\in \\mathcal{S}$  we have $\\mathbb{P}(s' \\vert s, a) = 1$, then for all $s'' \\in \\mathcal{S} \\backslash \\lbrace s'\\rbrace$ we have $\\mathbb{P}(s'' \\vert s, a) = 0$,\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles.    \n",
    "   - If at state $s$, taking action $a$, leads to a wall or an obstacle then $r(s,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to some other position in the maze that is not the exit nor a wall nor an obstacle, then $r(s, a) = -1$. \n",
    "   - If at state $s$, taking action $a$, leads to the exit then $r(s ,a) = 0$. \n",
    "> **Note**: Here the rewards are independent of time (i.e. $r_t(.,.) = r(.,.)$). \n",
    "\n",
    "\n",
    "### Implementation\n",
    "The above MDP formulation is implemented as a class ``maze.Maze`` in the file [maze.py](./maze.py) which given a matrix description of the maze instanciates the state space, action space, transition probabilities and rewards. \n",
    "\n",
    "> **Note:** In the class `maze.Maze` each state $s = (i,j)$ is given a unique identifier $s_{id} \\in \\lbrace 0, , \\dots, \\vert S \\vert -1 \\rbrace$. In other words, the state space from an implementation perspective is viewed as the set of integers $\\lbrace 0, , \\dots, \\vert S \\vert -1 \\rbrace$. This mapping is done via the dictionary `self.map` and its inverse mapping via the dictionary `self.states`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "env = mz.Maze(maze)\n",
    "# env.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dynamic Programming \n",
    "\n",
    "Before solving the MDP problem, recall that the finite horizon objective function is \n",
    "$$\n",
    "    \\mathbb{E} \\Big[ \\sum_{t=0}^T r(s_t, a_t) \\Big],\n",
    "$$\n",
    "where $T$ is the horizon.\n",
    "Recall the Bellman equation \n",
    "\\begin{equation}\n",
    "\\forall s \\in \\mathcal{S} \\qquad  V(s) = \\max_{a \\in \\mathcal{A}} \\Big\\lbrace r(s,a) + \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s'\\vert s,a) V(s') \\Big\\rbrace\n",
    "\\end{equation}\n",
    "The dynamic programming solution for the finite horizon MDP problem consists of solving the above backward recursion. The method `maze.dynamic_programming` achieves this. \n",
    "> **Note:** To find the optimal path, it is enough to set the time horizon $T = 10$. Indeed, looking at the maze one can see that the player needs at least 10 steps to attain the exit $B$, if her starting position is at $A$. In fact if you set the time horizon less than 10, you will see that you do not find the optimal path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 10\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env,horizon);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 0, 6, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c2abdfc1e091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DynProg'\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/matteo/RL_labs/lab0/maze.py\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(self, start, policy, method)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# Initialize current state and time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;31m# Add the starting position in the maze to the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0, 6, 5)"
     ]
    }
   ],
   "source": [
    "# Simulate the shortest path starting from position A\n",
    "method = 'DynProg';\n",
    "start  = (0,0,6,5);\n",
    "path = env.simulate(start, policy, method);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Here we solve the discounted infinite-horizon MDP problem using value iteration, the objective here is to find a stationary policy $\\pi$ that minimizes the infinite horizon objective with a discount factor $\\gamma$ \n",
    "$$\n",
    "    \\mathbb{E} \\Big[\\sum_{t=0}^\\infty \\gamma^t r\\big(s_t, \\pi(s_t)\\big) \\Big].\n",
    "$$\n",
    "Recall the Bellman equation in the case of a stationary policy $\\pi$ \n",
    "\\begin{equation}\n",
    "\\forall s \\in \\mathcal{S} \\qquad  V^*(s) = \\max_{\\pi} \\Big\\lbrace r(s,\\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s'\\vert s,\\pi(s)) V^*(s') \\Big\\rbrace\n",
    "\\end{equation}\n",
    "or equivalently in terms of the Bellman operator $\\mathcal{L}$ \n",
    "\\begin{equation}\n",
    "V^* =  \\mathcal{L}(V^*)\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "   \\forall s \\in \\mathcal{S} \\qquad  \\mathcal{L}(V)(s) = \\max_{\\pi} \\Big\\lbrace r(s,\\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s'\\vert s,\\pi(s)) V(s') \\Big\\rbrace. \n",
    "\\end{equation}\n",
    "Value iteration solves the Bellman equation described above. This method is implemented as `maze.value_iteration` in the file [maze.py]().\n",
    "\n",
    "> **Note:** Recall that the mapping $\\mathcal{L}$ is a contraction, therefore value iteration converges. To achieve an $\\varepsilon>0$ approximation (i.e. $\\Vert V^* - V_{n+1} \\Vert \\le \\varepsilon$),\n",
    " the stopping criterion of value iteration is $\\Vert V - \\mathcal{L}(V) \\Vert < \\frac{1-\\gamma}{\\gamma}\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discount Factor \n",
    "gamma   = 0.95; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.0001;\n",
    "V, policy = mz.value_iteration(env, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 'ValIter';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random rewards \n",
    "\n",
    "### The new MDP formulation \n",
    "As stated in the problem statement, we only modify the rewards $\\mathcal{R}$ to be random. In fact we will only need to modify the rewards corresponding to the state action pair $(s,a)$ that lead to either the cell R1 or R2.\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles.    \n",
    "   - If at state $s$, taking action $a$, leads to the cell R1 then the reward is random according to the following     $$ R(s,a) = \\begin{cases}\n",
    "            -7 \\quad \\textrm{ w.p. } 0.5 \\\\\n",
    "            -1 \\quad \\textrm{ w.p. } 0.5\n",
    "            \\end{cases} \n",
    "     $$\n",
    "   - If at state $s$, taking action $a$, leads to the cell R2 then the reward is random according to the following \n",
    "     $$ R(s,a) = \\begin{cases}\n",
    "            -2 \\quad \\textrm{ w.p. } 0.5 \\\\\n",
    "            -1 \\quad \\textrm{ w.p. } 0.5\n",
    "            \\end{cases} \n",
    "     $$\n",
    "   - The remaining rewards remain deterministic and with the same values as in the previous formulation.\n",
    "\n",
    "> **Note**: The fact that you stay in a cell for a number of rounds $n$ means that you are forced to incur the reward of ending up in that state for an additional $n$ times. Thus, instead of modifying the transition probabilities, we can modify the reward of ending up at that round by multiplying it by $n + 1$.  \n",
    "\n",
    "### Solving the new MDP \n",
    "As mentioned in the appendix [random_rewards.pdf]() (see in canvas), when solving the problem we will only have to look at the average rewards instead of the realization of the rewards, and the methods implemented for the previous case remain unchanged.  \n",
    "\n",
    "> **Note**: In the implementation, the only change will be the rewards. In addition, the policies we obtain remain deterministic. However, when running a policy the accumulated reward is random, but its average over multiple repetitions should converge to the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 0, 0, 0, 0, -1],\n",
    "    [ 0, 1, 1, 1, 1, 1,  0],\n",
    "    [-6, 0, 0, 0, 0, 2,  0]\n",
    "])\n",
    "# with the convention \n",
    "#  0 = empty cell\n",
    "#  1 = obstacle\n",
    "#  2 = exit of the Maze\n",
    "# -n = trapped cell with probability 0.5. If the cell is trapped the player must stay there for n times.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mz.draw_maze(maze);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic programming \n",
    "\n",
    "Run the following python code to obtain the optimal solution of the newly formulated MDP with dynamic programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "env = mz.Maze(maze, random_rewards=True)\n",
    "# env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 15\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env,horizon);\n",
    "# Simulate the shortest path starting from position A\n",
    "method = 'DynProg';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method);\n",
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The animation does not illustrate the event where the player is trapped as it assumes average rewards. Nonetheless, the shown policy is the optimal one.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration  \n",
    "\n",
    "Run the following python code to obtain the optimal solution of the newly formulated MDP with value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discount Factor \n",
    "gamma   = 0.95; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.0001;\n",
    "V, policy = mz.value_iteration(env, gamma, epsilon)\n",
    "\n",
    "method = 'ValIter';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method)\n",
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 : Plucking berries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The new MDP formulation \n",
    "\n",
    "In this problem, the introduction of weights is translated in our previous MDP formulation by a modification of the rewards $\\mathcal{R}$. This is done by simply setting $r(s,a)$ to $w_{ij}$ if being in state $s$ and taking action $a$ leads to being in th new state $s'=(i,j)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 0, 0, 0, 0,  0],\n",
    "    [ 0, 1, 1, 1, 1, 1,  0],\n",
    "    [ 0, 0, 0, 0, 0, 2,  0]\n",
    "])\n",
    "\n",
    "# Description of the weight matrix as a numpy array\n",
    "w = np.array([\n",
    "    [0,    1, -100,   10,   10,   10, 10],\n",
    "    [0,    1, -100,   10,    0,    0, 10],\n",
    "    [0,    1, -100,   10,    0,    0, 10],\n",
    "    [0,    1,    1,    1,    0,    0, 10],\n",
    "    [0, -100, -100, -100, -100, -100, 10],\n",
    "    [0,    0,    0,    0,    0,   11, 10]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "env = mz.Maze(maze, weights=w)\n",
    "# env.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic programming \n",
    "\n",
    "Run the following python code to obtain the optimal solution of the newly formulated MDP with dynamic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 20\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env,horizon);\n",
    "# Simulate the shortest path starting from position A\n",
    "method = 'DynProg';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method);\n",
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** By changing the horizon from $20$ to $12$ you should observe that the optimal policy changes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration  \n",
    "\n",
    "Run the following python code to obtain the optimal solution of the newly formulated MDP with value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discount Factor \n",
    "gamma   = 0.50; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.001;\n",
    "V, policy = mz.value_iteration(env, gamma, epsilon)\n",
    "method = 'ValIter';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method)\n",
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}